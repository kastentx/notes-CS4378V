# Semi-Supervised Learning
* labelled data is nice becuase it helps with training
  * ex: images of cats and dogs, seperated into folders
* most of the time, a lot of the data you'll work with is unlabeled
  * ex: images of cats and dogs mixed together

## Intro
* supervised means all data has labels
* semi-supervised means some data is labelled, some unlabelled
  * **Transductive Learning:** unlabeled data is the testing data
  * **Inductive Learning:** unlabeled data is _not_ the testing data
* collecting data is easy but labelled data is expensive/hard to acquire
* we do semi-supervised learning in our daily lives
  * when we learn what a new thing is, we get the label
* we use assumptions when working with unlabelled data

## Semi-Supervised Learning for Generative Model
* Bayes probability example (Two Boxes)
  * can be seen as Two Classes, instead of boxes

## Supervised Generative Model
* given labelled training examples..
  * look for most likely prior probability and class-dependant probability

## Semi-Supervised Generative Model
* given labelled training examples..
  * look for most likely prior probability and class-dependant probability
* unlabeled data helps re-estimate the probabilities, means, and variance
* `E` step 1: compute the posterior probability of unlabeled data
* `M` step 2: update model
* repeat until convergence
* this method is somewhat outdated, very sensitive to unlabeled data

## Semi-Supervised Learning (Low-Density Separation)
* self-training
  * train a model on the labelled data-set
  * apply the model to the unlabeled data-set (gets Pseudo-label)
  * remove the data from "unlabeled" and move to "labeled"
    * how to choose which data to use has many choices
* Hard-label vs. Soft-label
  * soft labels can give a percentage chance for each class/label
  * hard labels need to be used, which convert to a binary-based label
    * one choice, one label.. no multiple labels
    * needed to update model iteratively
* **Entropy Based Regularization**
  * evaluates the concentration of the distribution of y<sup>u</sup>

### Semi-supervised SVM (Outlook)
* _"Transductive Inference for Text Classification using Support Vector Machines"_ ICML 1999
* finding a decision boundary that provides the largest margin and least error

## Smoothness Assumption
* assume: similar _x_ should have the same _y_
  * x is not uniform
  * if x<sup>1</sup> and x<sup>2</sup> are close in a high-density region, they should have the same label
* tutorial slides ( byxiaojin zhu)
  * "indirectly" similar objects, looks better with stepping stones in between
* Cluster and then label the data

### Graph-based approach
* represent data points with a _graph_
  * showing the connections between datapoints
    * sometimes the connections are naturally there
    * sometimes you have to construct the graph
      * define the similarity between datapoints
        * add edges with K-nearest neighbor, e-neighborhood
        * edge weight proportional to the defined similarity
        * can use Gaussian Radial Basis function
      * labelled data propogates throughout the graph
* smoothness of labels on a graph can be determined quantitatively (with a function)
  * smaller output from this equation means a smoother graph
* **y**: (R+U) dimension vector
* **L**: (R+U) x (R+U) matrix _(Graph Laplacian)_
  * L = D - W (two different matricies)
* _"Deep Learning via semi-supervised embedding"_ ICML 2008
* 