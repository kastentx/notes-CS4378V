# Model Selection
* to predict a house selling price
  * first, collect some data
    * different houses that have sold in area, with selling prices
  * then, identify features
    * size, num_bedrooms, price_sold
  * identify the features for your house
  * pick ML algorithm to use
    * (linear regression)
      * f(x) = theta0 + theta1x
    * what about quadratic? polynomial?
    * check "error" for each function
      * lowest amount of error - not necessarily best
      * danger of overfitting
        * a perfect fit on training data could mean future data doesn't fit

## How to do it properly
* don't use _all_ the training data to train your model
  * split some of it off to use as _test data_, and some as _train data_

## Model Selection
* use different algorithms, use different equations
* find something that gives you lowest error on test data thru **cross-validation**

### Hold-Out Cross Validation
1. randomly split data into two parts
  * one part to train and one part to test (about 70/30)
    * test set called _hold-out cross validation set_
2. train each model of train set only, to get some hypothesis
3. Select and output the hypothesis that had the smallest error

* problem -> this makes your dataset smaller than it already is
* there are other methods of Cross Validation that are better suited for smaller datasets

### K-fold Cross Validation
1. divide the (randomized) dataset into `k` partitions
2. the process will run `k` times
3. each time, a different partition will be selected to use as the test data
  * this way, each sample gets used for training, and you don't lose out on any data
4. in practice, people will commonly use a value of 10 for `k`

#### using k-fold for model selection
1. randomly split dataset S into k (disjoint) subsets of equal size
  * these are called S1...Sk
2. for each model Mi, we evaluate it as follows:
  * for j=1 to j=k
  * train on all the data except Sj to get some hypothesis h<sub>ij</sub>
3. the estimated generalization error of model Mi is the average of errors for each j
4. pick the model Mi with the lowest estimated generalization error
  * this model would get retrained on the entire dataset! now its the best

### Leave one out Cross Validation
* kind of like k-fold, but k is equal to the number of data samples
* each time, only one sample is used for testing, and the rest for training
* e.g. use 1-99 for training, and test with sample number 100

## Feature Selection
* which features are going to correlate best with the desired output/result
* we want to select the best ones, and leave out 'noisy' ones
* in computer vision.. this could be the process of picking which pixels to analyze
* if you have n-features.. there are 2<sup>n</sup> combinations of features to use
* this is way too many to brute-force.. there needs to be a sytematic way to test

1. Forward Search
  1. initialize with an empty set (F)
  2. for i=1 to n... try adding feature i to F
  3. evaluate the model with cross-validation
  4. add the best feature found to the set F
  5. to stop, there are several different methods
    * one way is to use a set number of max features
2. Backward Search
  1. initialize with set containing all features (F)
  2. each time, find the best feature to remove!
  3. otherwise, its the same as forward search
3. statistical/correlation way
  * find the correlation between each feature and the output
  * 