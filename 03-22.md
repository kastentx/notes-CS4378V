## RMSProp
* like a more advanced version of gradient descent
  * allows you to have an adaptive learning rate
* in physical world, a ball rolling down a graph has _momentum_
  * how does this apply in gradient descent?

## Momentum
* Movement: movement of last step minus gradient at present
  * based not just on gradient but previous movement as well
* still not a guaranteee of reaching global minima, but it helps..

## Early Stopping

## Dropout
* before updating parameters in a Nerual Network, each neuron has a p% to dropout
  * this causes the structure of network to change -> becomes thinner
* dropped out neurons can be resampled in each mini-batch
* intuitive reason for this: 
  * on a team project.. each team member thinks the others will not do work (dropout)
  * in reality, if everyone does their work, the team's performance will improve
* when testing.. make sure to multiply the weights of paths between neurons by dropout rate
* example:
  * imagine a 2 neuron network
  * 2^2 = 4, so there are 4 possible networks with dropout
    * 1 will include both neurons
    * 1 will include neither neuron
    * 2 will include only 1 neuron
  * summing the output of these neurons and dividing by 4 give the avg output
    * this will be the same as adding all the neurons up and multiplying their weights by (1-p)

## Regularization


# Next week
* we will talk about semi-supervised learning
  * it is very costly to acquire labels for data