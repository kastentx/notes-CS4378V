# Gradient Descent
* **Gradient** is the slope of a function
* **Optimization** is finding the "best" value of a function which is the minimum value of the function
* The number of "turning points" of a function depend on the order of the function
* if function is not convex or concave, we can only find local mins and max
* "repeat until convergence"

## Gradient Descent for Linear Regression
* algorithm: Ɵ<sub>j</sub> := Ɵ<sub>j</sub> - α (a/aƟ<sub>j</sub>)J(Ɵ<sub>0</sub>,Ɵ<sub>1</sub>)
* learning rate steps each point down to convergence
  * if too large a learning rate is chosen, they won't converge and func value may increase
* if J(Ɵ<sub>0</sub>,Ɵ<sub>1</sub>) = 0, then the cost function fits the data perfectly
  * new data might not fit as well

## Multivariate Linear Regression
* x<sup>i</sup> - ith training sample
* x<sub>j</sub> - jth feature
* n - number of features
* m - number of training samples
* Ɵ and x can be written as vectors
  * Ɵ has size n+1
  * x has size n+1
* h<sub>Ɵ</sub>(x) = Ɵ<sup>T</sup>x
  * the exponent t means 'transposed'
  * the exponent -1 means 'inverse of'
* identity matrix?