# Transfer Learning (with labelled data)
* fix some layers, and use new data to retrain only some of the layers
* used for model fine tuning

# MultiTask Learning
* multi-layer structure that makes it possible to use NN on applications with multiple tasks
  * for example, multilingual speech recognition
  * error rates decrease when more training data is added... even if its from different languages

## Progressive Neural Network
* output of first layer becomes input for second layer in different NN

# Transfer Learning (with unlabelled target data)
* source data has labels (MNIST)
* target data has no labels (MNIST-M)

## Domain-adversarial Training
* same as CNN.. good to use for image classification
* plot all the data in a 2D domain
  * in slides.. blue dots are MNIST, and red dots are MNIST-M
  * distributions of data are very different.. so not great for transfer learning
    * this is where domain-adversarial comes in
  * after some procedure, the distributions can be made more similar
    * uses a feature extractor and a domain classifier
* simliar to `GAN` (another popular type of NN)
* goal is to _not only cheat the domain classifier, but satisfy label predictor as well_
  * also to maximize label classfication accuracy but minimize domain classification accuracy
* gradient descent is still used (gradient reverse layer is also used)


* 3 parts:
  * feature extractor
    * input from 2 diff domains will be very different
  * domain classifier
    * will determine whether input is from source or target domain
  * label predictor
    * predict accurate labels for all images
* each of these have their own goal.. but the system overall has a difffernt goal that can be at odds with components

## Zero-Shot Learning
* represent each class by its attributes in a 'huge table' <- table might be imaginary
  * ex: furry?, 4 legs?, tail?
  * collect enough attributes for one to one mapping
* binary output for each one of these attributes
* during testing
* train on enough data to recognize all the attributes
  * now, you can test on classes that haven't been trained, but use existing attributes
* attribute embedding
  * embed attributes from 'image space' into 'embedding space'
  * attribute embedding + word embedding
* ... what if there's no database?
  * math explained in the slides
  * use 'convex combination of semantic embedding'
    * finding word vectors and combining them
      * .5(lion) + .5(tiger) = liger
* this techinique is used by google to translate between new languages

# unlabelled(source) w/ labelled(target)
## self-taught learning
* learning to extract better representation from the source data
* extracting better representation for taget data

# unlabelled source + target
* Self-Taught Clustering

* this is all different from semi-supervised
  * in semi-supervised the data should be from one domain only
* 