# Neural Network

1. need to check if results from Training data are good
  * No? -> try new activation function 
  * Yes? -> Go on to Testing data
2. check if results from Testing data are good
  * No? -> Overfitting
  * Yes? -> You're good
* Follow the "Recipe of Deep Learning" from lecture slides
* Deeper layers doesn't imply _better_
  * if there's an error, that will just get worse with more layers
* Rectified Linear Unit (ReLU) can be used to solve "Vanishing Gradient Problem"

## Activation Functions
* Sigmoid
* ReLU
  * special case of maxout
* Leaky ReLU
* Maxout
* 


* GAN network 
  * generative adversarial network
  * developed by Ian Goodfellow